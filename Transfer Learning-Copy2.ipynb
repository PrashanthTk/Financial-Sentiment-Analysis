{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getpreppedinputs( dev_texts, dev_labels,max_length=300, by_sentence=True):\n",
    "    nlp = spacy.load('en_vectors_web_lg')\n",
    "    nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "    embeddings = get_embeddings(nlp.vocab)\n",
    "\n",
    "    \n",
    "    #model = compile_lstm(embeddings, lstm_shape, lstm_settings)\n",
    "    print(\"Parsing texts...\")\n",
    "    #train_docs = list(nlp.pipe(train_texts))\n",
    "    dev_docs = list(nlp.pipe(dev_texts))\n",
    "    if by_sentence:\n",
    "        #train_docs, train_labels = get_labelled_sentences(train_docs, train_labels)\n",
    "        dev_docs, dev_labels = get_labelled_sentences(dev_docs, dev_labels)\n",
    "\n",
    "    #train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "    dev_X = get_features(dev_docs, max_length)\n",
    "    return dev_X,dev_labels\n",
    "\n",
    "\n",
    "#Dont need to use get_labelled_sentences if Im using readALlfiles\n",
    "def readAllFiles(dirpath='Call-Abstractor/fullText',max_length=300):\n",
    "    sentences=[]\n",
    "    labels=[]\n",
    "    for file in glob.glob(path):\n",
    "        text=open(file,'r').read()\n",
    "        doc=nlp(text)\n",
    "        for sent in doc.sents:\n",
    "            sentences.append(sent.text)\n",
    "            labels.append(random.randint(0,4))\n",
    "    sentences=get_features(sentences,max_length)\n",
    "    return sentences,numpy.asarray(labels,dtype='int32')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'rand' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-acafa45163c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'rand' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, train_labels =read_AllFiles(dirpath='Call-Abstractor/taggedCalls/2Calls.csv')\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#data, labels = np.arange(10).reshape((5, 2)), range(5)\n",
    "\n",
    "train_texts, dev_texts, train_labels, dev_labels = train_test_split(train_texts, train_labels, test_size=0.20)\n",
    "lstm = train(train_texts, train_labels, dev_texts, dev_labels,\n",
    "                 {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': 5},\n",
    "                 {'dropout': dropout, 'lr': learn_rate},\n",
    "                 {},\n",
    "                 nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "weights = lstm.get_weights()\n",
    "lstm.save('Call-Abstractor/model/imdbcaller.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This example shows how to use an LSTM sentiment classification model trained using Keras in spaCy. spaCy splits the document into sentences, and each sentence is classified using the LSTM. The scores for the sentences are then aggregated to give the document score. This kind of hierarchical model is quite difficult in \"pure\" Keras or Tensorflow, but it's very effective. The Keras example on this dataset performs quite poorly, because it cuts off the documents so that they're a fixed size. This hurts review accuracy a lot, because people often summarise their rating in the final sentence\n",
    "Prerequisites:\n",
    "spacy download en_vectors_web_lg\n",
    "pip install keras==2.0.9\n",
    "Compatible with: spaCy v2.0.0+\n",
    "\"\"\"\n",
    "import spacy\n",
    "import re\n",
    "from sense2vec import Sense2VecComponent\n",
    "nlps2v = spacy.load('en')  # make sure to use larger model!\n",
    "s2v=Sense2VecComponent('reddit_vectors-1.1.0')\n",
    "nlps2v.add_pipe(s2v)\n",
    "nlp = spacy.load('en_vectors_web_lg')\n",
    "nlp.add_pipe(nlp.create_pipe('sentencizer'))\n",
    "\n",
    "import getFinSent\n",
    "import plac\n",
    "import random,pickle\n",
    "import pathlib\n",
    "import cytoolz\n",
    "from keras.utils import to_categorical\n",
    "import numpy\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential, model_from_json\n",
    "from keras.layers import LSTM, Dense, Embedding, Bidirectional\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.optimizers import Adam\n",
    "import thinc.extra.datasets\n",
    "from spacy.compat import pickle\n",
    "import spacy\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "import glob\n",
    "from random import randint\n",
    "posList=getFinSent.loadPositive()\n",
    "negList=getFinSent.loadNegative()\n",
    "class SentimentAnalyser(object):\n",
    "    @classmethod\n",
    "    def load(cls, path, embeddings,nlp, max_length=100):\n",
    "        \"\"\"if path!=\"default\":\n",
    "            path=('../TransLearn/models')\"\"\"\n",
    "        with (path / 'config.json').open() as file_:\n",
    "            model = model_from_json(file_.read())\n",
    "        with (path / 'model').open('rb') as file_:\n",
    "            lstm_weights = pickle.load(file_)\n",
    "        if embeddings is None:\n",
    "            embeddings = get_embeddings(nlp.vocab)\n",
    "        model.set_weights([embeddings] + lstm_weights)\n",
    "        return cls(model, max_length=max_length)\n",
    "\n",
    "    def __init__(self, model, max_length=100):\n",
    "        self._model = model\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __call__(self, doc):\n",
    "        X = get_features([doc], self.max_length)\n",
    "        y = self._model.predict(X)\n",
    "        self.set_sentiment(doc, y)\n",
    "\n",
    "    def pipe(self, docs, batch_size=1000, n_threads=2):\n",
    "        for minibatch in cytoolz.partition_all(batch_size, docs):\n",
    "            minibatch = list(minibatch)\n",
    "            sentences = []\n",
    "            for doc in minibatch:\n",
    "                sentences.extend(doc.sents)\n",
    "            Xs = get_features(sentences, self.max_length)\n",
    "            ys = self._model.predict(Xs)\n",
    "            for sent, label in zip(sentences, ys):\n",
    "                sent.doc.sentiment += label - 0.5\n",
    "            for doc in minibatch:\n",
    "                yield doc\n",
    "\n",
    "    def set_sentiment(self, doc, y):\n",
    "        doc.sentiment = float(y[0])\n",
    "        # Sentiment has a native slot for a single float.\n",
    "        # For arbitrary data storage, there's:\n",
    "        # doc.user_data['my_data'] = y\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def get_labelled_sentences(docs, doc_labels):\n",
    "    \n",
    "    #This function has been changed from the original\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    for doc, y in zip(docs, doc_labels):\n",
    "        \n",
    "        sentences.append(doc)\n",
    "        if y<-1:\n",
    "            onehot=0\n",
    "        elif y<0:\n",
    "            onehot=1\n",
    "        elif y==0:\n",
    "            onehot=2\n",
    "        elif y<=1:\n",
    "            onehot=3\n",
    "        elif y>1:\n",
    "            onehot=4\n",
    "        else:\n",
    "            print('Error while encoding label')\n",
    "                \n",
    "            \n",
    "        labels.append(onehot)\n",
    "    newlabels=to_categorical(labels,5)\n",
    "    #print('Some labels are'+str(newlabels[:20]))\n",
    "            \n",
    "    #return sentences, numpy.asarray(labels, dtype='int32')\n",
    "    return sentences, newlabels\n",
    "#Return matrix representation of all sentences embedded with token ids. Sentence will look like [1,192,3,6...]\n",
    "\n",
    "\n",
    "def get_features(docs, max_length):\n",
    "    docs = list(docs)\n",
    "    Xs = numpy.zeros((len(docs), max_length), dtype='int32')\n",
    "    c=0\n",
    "    #doc is a string. A sentence text\n",
    "    for i, doc in enumerate(docs):\n",
    "        j = 0\n",
    "        if c<30:\n",
    "            c+=1\n",
    "            #print(\"Inside get_features. doc is\"+str(doc)+\" \\n Type of doc is\"+str(type(doc)))\n",
    "        breakdoc=nlp(doc)\n",
    "        for token in breakdoc:\n",
    "            #print(type(token))\n",
    "            vector_id = token.vocab.vectors.find(key=token.orth)\n",
    "            if vector_id >= 0:\n",
    "                Xs[i, j] = vector_id\n",
    "            else:\n",
    "                Xs[i, j] = 0\n",
    "            j += 1\n",
    "            if j >= max_length:\n",
    "                break\n",
    "    #print(\"Inside get_features. Some vector IDS\")\n",
    "    print(Xs[0, 10])\n",
    "    print(Xs[1,39])\n",
    "    \n",
    "    return Xs\n",
    "\n",
    "\n",
    "def compile_lstm(embeddings, shape, settings,trainable=True):\n",
    "    model = Sequential()\n",
    "    model.add(\n",
    "        Embedding(\n",
    "            embeddings.shape[0],\n",
    "            embeddings.shape[1],\n",
    "            input_length=shape['max_length'],\n",
    "            trainable=trainable,\n",
    "            weights=[embeddings],\n",
    "            mask_zero=True\n",
    "        )\n",
    "    )\n",
    "    model.add(TimeDistributed(Dense(shape['nr_hidden'], use_bias=False)))\n",
    "    model.add(Bidirectional(LSTM(shape['nr_hidden'],\n",
    "                                 recurrent_dropout=settings['dropout'],\n",
    "                                 dropout=settings['dropout'])))\n",
    "    model.add(Dense(shape['nr_class'], activation='softmax'))\n",
    "    model.compile(optimizer=Adam(lr=settings['lr']), loss='categorical_crossentropy',\n",
    "          metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model,embeddings\n",
    "\n",
    "\n",
    "def get_embeddings(vocab):\n",
    "    return vocab.vectors.data\n",
    "\n",
    "\n",
    "def evaluate(model_dir, texts, labels, max_length=100):\n",
    "    def create_pipeline(nlp):\n",
    "        '''\n",
    "        This could be a lambda, but named functions are easier to read in Python.\n",
    "        '''\n",
    "        return [nlp.tagger, nlp.parser, SentimentAnalyser.load(model_dir, nlp,\n",
    "                                                               max_length=max_length)]\n",
    "\n",
    "    \"\"\"nlp = spacy.load('en')\n",
    "    nlp.pipeline = create_pipeline(nlp)\"\"\"\n",
    "\n",
    "    correct = 0\n",
    "    i = 0\n",
    "    for doc in nlp.pipe(texts, batch_size=1000, n_threads=4):\n",
    "        correct += bool(doc.sentiment >= 0.5) == bool(labels[i])\n",
    "        i += 1\n",
    "    return float(correct) / i\n",
    "\n",
    "#Only used for the initial phase of Transfer Learning\n",
    "def read_data(data_dir, limit=0):\n",
    "    examples = []\n",
    "    \"\"\"for subdir, label in (('pos', 1), ('neg', 0)):\n",
    "        for filename in (data_dir / subdir).iterdir():\n",
    "            with filename.open() as file_:\n",
    "                text = file_.read()\n",
    "            examples.append((text, label))\n",
    "    random.shuffle(examples)\"\"\"\n",
    "    c=0\n",
    "    #nlp.add_pipe(s2v)\n",
    "    #print(data_dir)\n",
    "    texts=[]\n",
    "    for filename in glob.glob(data_dir+\"*.txt\"):\n",
    "        #print(filename)\n",
    "        file=open(filename,'r')\n",
    "        ftext = file.read()\n",
    "        #==================\n",
    "        texts.append(ftext)\n",
    "        \n",
    "\n",
    "        \n",
    "        #=====================\n",
    "        docs=nlps2v(ftext)\n",
    "        for sent in docs.sents:\n",
    "            stext=sent.text\n",
    "            cleanText=[]\n",
    "            s2vsent = nlps2v(stext)\n",
    "            # Break sentence into list of tokens\n",
    "            for token in s2vsent:\n",
    "                cleanText.append(token.text)\n",
    "            label=getFinSent.getSentiment(cleanText,negList,posList)\n",
    "            #print(\"Inside read_data\"+str(label))\n",
    "            #if label!=0:\n",
    "                \n",
    "            #ds\n",
    "            #print(stext+'!!!!!!!!!!!! Label'+str(label))\n",
    "            examples.append((stext,label))\n",
    "            \n",
    "    #nlp.remove_pipe('sense2vec')\n",
    "    \"\"\"if limit >= 1:\n",
    "        examples = examples[:]\"\"\"\n",
    "    #======\n",
    "    \n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    MAX_NB_WORDS=50000\n",
    "    tokenizer = Tokenizer(nb_words=MAX_NB_WORDS)\n",
    "    tokenizer.fit_on_texts(texts)\n",
    "    sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "    word_index = tokenizer.word_index\n",
    "    print('Found %s unique tokens.' % len(word_index))\n",
    "    #======\n",
    "    #print(\"Inside read_data,  examples\"+str(examples))\n",
    "    return zip(*examples) # Unzips into two lists\n",
    "\n",
    "def train(train_texts, train_labels, dev_texts, dev_labels,\n",
    "          lstm_shape, lstm_settings, lstm_optimizer, batch_size=100,\n",
    "          nb_epoch=5, by_sentence=True,model=None,embeddings=None,trainable=True,labels=\"-5to5\",usinginputs=False):\n",
    "    print(\"Loading spaCy\")\n",
    "    \n",
    "    \n",
    "    if model is None and embeddings is None:\n",
    "        \n",
    "        embeddings = get_embeddings(nlp.vocab)\n",
    "        print(\"embeddings shape is \"+str(embeddings.shape))\n",
    "        model,embeddings = compile_lstm(embeddings, lstm_shape, lstm_settings,trainable=trainable)\n",
    "    if not usinginputs:\n",
    "        print(\"Parsing texts...\")\n",
    "\n",
    "        #train_docs = list(nlp.pipe(train_texts))\n",
    "        #dev_docs = list(nlp.pipe(dev_texts))\n",
    "\n",
    "        train_docs=train_texts\n",
    "        #dev_docs=dev_texts\n",
    "        #print(\"Inside train, Length of train_texts is\"+str(len(train_texts)))\n",
    "        print(\"One sample is \"+str(train_texts[0])+\" And type is\"+str(type(train_texts[0])))\n",
    "        #print(\"Labels are\"+str(train_labels))\n",
    "        if by_sentence:\n",
    "            if(labels==\"-5to5\"):\n",
    "                train_docs, train_labels = get_labelled_sentences(train_texts, train_labels)\n",
    "                #dev_docs, dev_labels = get_labelled_sentences(dev_texts, dev_labels)\n",
    "            elif(labels==\"0to1\"):\n",
    "                pass\n",
    "            else:\n",
    "                train_labels=list(train_labels)\n",
    "                train_labels=to_categorical(train_labels,5)\n",
    "                #Assume the labeling is already in 0to4 format\n",
    "        print(\"Received train_labels from get functions  are \"+str(train_labels))\n",
    "        print(\"Inside train, Length of train_docs is\"+str(len(train_docs)))\n",
    "        train_X = get_features(train_docs, lstm_shape['max_length'])\n",
    "\n",
    "        #dev_X = get_features(dev_docs, lstm_shape['max_length'])\n",
    "        print(\"Inside train, Shape of dev_X is\"+str(dev_X.shape))\n",
    "        print(\"One devX sampe is \"+str(dev_X[0]))\n",
    "        print(train_X.shape)\n",
    "        print(train_labels.shape)\n",
    "        print('Train_labels 2 of them are'+str(train_labels[:2]))\n",
    "        print('devlabels 2 of them are'+str(dev_labels[:2]))\n",
    "        print('Inside train...')\n",
    "        #print(train_X[0])\n",
    "        #print(\"Labels are\"+str(train_labels))\n",
    "        pickle.dump(train_X,open(\"../TransLearn/2calls/bwgbctrain_X.p\",\"wb\"))\n",
    "        pickle.dump(train_labels,open(\"../TransLearn/2calls/bwgbctrain_labels.p\",\"wb\"))\n",
    "        pickle.dump(dev_X,open(\"../TransLearn/2calls/bwgbcdev_X.p\",\"wb\"))\n",
    "        pickle.dump(dev_labels,open(\"../TransLearn/2calls/bwgbcdev_labels.p\",\"wb\"))\n",
    "    else:\n",
    "        train_X=pickle.load(open(\"../TransLearn/2calls/bctrain_X.p\",\"rb\"))\n",
    "        train_labels=pickle.load(open(\"../TransLearn/2calls/bctrain_labels.p\",\"rb\"))\n",
    "    history=model.fit(train_X, train_labels, validation_split=0.3,\n",
    "              nb_epoch=nb_epoch, batch_size=batch_size)\n",
    "    \n",
    "    print('Shape of embeddings'+str(embeddings.shape))\n",
    "    pickle.dump(embeddings,open('../TransLearn/2calls/BWGRotEmbeddings.p','wb'))\n",
    "    return model,history\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/preprocessing/text.py:172: UserWarning: The `nb_words` argument in `Tokenizer` has been renamed `num_words`.\n",
      "  warnings.warn('The `nb_words` argument in `Tokenizer` '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 23319 unique tokens.\n",
      "Inside main function, labels are<class 'tuple'>\n",
      "Found 15744 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "model_dir=None\n",
    "train_dir=None\n",
    "dev_dir=None\n",
    "is_runtime=False\n",
    "nr_hidden=256\n",
    "max_length=50; # Shape\n",
    "dropout=0.5\n",
    "learn_rate=0.001; # General NN config\n",
    "nb_epoch=2\n",
    "batch_size=64\n",
    "nr_examples=2000 # Training params\n",
    "\"\"\"if model_dir is not None:\n",
    "    model_dir = pathlib.Path(model_dir)\n",
    "if train_dir is None or dev_dir is None:\n",
    "    imdb_data = thinc.extra.datasets.imdb()\n",
    "if is_runtime:\n",
    "    if dev_dir is None:\n",
    "        dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "    else:\n",
    "        dev_texts, dev_labels = read_data(dev_dir)\n",
    "    acc = evaluate(model_dir, dev_texts, dev_labels, max_length=max_length)\n",
    "    print(acc)\n",
    "else:\"\"\"\n",
    "train_dir=\"fullText/\";dev_dir=\"dev_dir/\";model_dir=\"glove/\"\n",
    "\n",
    "\n",
    "if train_dir is None:\n",
    "    train_texts, train_labels = zip(*imdb_data[0])\n",
    "else:\n",
    "    print(\"Read data\")\n",
    "    #Get a list of Spacy sent objects intact and unbroken and their labels\n",
    "    train_texts, train_labels = read_data(train_dir, limit=nr_examples)\n",
    "    print(\"Inside main function, labels are\"+str(type(train_labels)))\n",
    "if dev_dir is None:\n",
    "    dev_texts, dev_labels = zip(*imdb_data[1])\n",
    "else:\n",
    "    dev_texts, dev_labels = read_data(dev_dir, limit=nr_examples)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading spaCy\n",
      "embeddings shape is (1070971, 300)\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           321291300 \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 256)           76800     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               1050624   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 322,421,289\n",
      "Trainable params: 322,421,289\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Parsing texts...\n",
      "One sample is welcome to the bwg strategy Conference Center please enter your user ID the conference will be getting it when the host arrives the conference is now in presentation mode you're lying is muted SPS Commerce are they gaining or losing share versus the competitive said and why we have retailers wear those trading partners need to connect with him in the benefit of having the retailers on the qualities of their unbiased position on who they're training partner chooses they have an aggregated view And type is<class 'str'>\n",
      "Received train_labels from get functions  are [[0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 1. 0.]]\n",
      "Inside train, Length of train_docs is72010\n",
      "1548\n",
      "0\n",
      "1548\n",
      "0\n",
      "Inside train, Shape of dev_X is(26019, 50)\n",
      "One devX sampe is [  3073    128    125 875410   2831 203756   7339    964   3438    201\n",
      "   1548      0    136   2123    125    125   6603    225    158    529\n",
      "    143    255    125   4522 408481    125   6603    137    261    146\n",
      "   7054      0      0      0      0      0      0      0      0      0\n",
      "      0      0      0      0      0      0      0      0      0      0]\n",
      "(72010, 50)\n",
      "(72010, 5)\n",
      "Train_labels 2 of them are[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "devlabels 2 of them are[[0. 0. 1. 0. 0.]\n",
      " [0. 0. 1. 0. 0.]]\n",
      "Inside train...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n",
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 321291300 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50407 samples, validate on 21603 samples\n",
      "Epoch 1/2\n",
      "24576/50407 [=============>................] - ETA: 36:22 - loss: 0.4402 - acc: 0.8672"
     ]
    }
   ],
   "source": [
    "#train_labels = numpy.asarray(train_labels, dtype='int32')\n",
    "#dev_labels = numpy.asarray(dev_labels, dtype='int32')\n",
    "lstm,history = train(train_texts, train_labels, dev_texts, dev_labels,\n",
    "             {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': 5},\n",
    "             {'dropout': dropout, 'lr': learn_rate},\n",
    "             {},\n",
    "             nb_epoch=nb_epoch, batch_size=batch_size,by_sentence=True)\n",
    "weights = lstm.get_weights()\n",
    "lstm.save('../TransLearn/models/BWGTLModel.p')\n",
    "lstm.save_weights('../TransLearn/BWGTLModelweights.h5')\n",
    "\"\"\"if model_dir is not None:\n",
    "    with (model_dir / 'model').open('wb') as file_:\n",
    "        pickle.dump(weights[1:], file_)\n",
    "    with (model_dir / 'config.json').open('wb') as file_:\n",
    "        file_.write(lstm.to_json())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "lstmjson=lstm.to_json()\n",
    "with open('../TransLearn/models/PTconfig.json', 'w') as outfile:\n",
    "    json.dump(jsonstring, outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_4 (Embedding)      (None, 100, 300)          321291300 \n",
      "_________________________________________________________________\n",
      "time_distributed_4 (TimeDist (None, 100, 128)          38400     \n",
      "_________________________________________________________________\n",
      "bidirectional_4 (Bidirection (None, 256)               263168    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 5)                 1285      \n",
      "=================================================================\n",
      "Total params: 321,594,153\n",
      "Trainable params: 321,594,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n",
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 321291300 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50407 samples, validate on 21603 samples\n",
      "Epoch 1/2\n",
      "50407/50407 [==============================] - 3832s 76ms/step - loss: 0.3477 - acc: 0.8862 - val_loss: 0.1825 - val_acc: 0.9227\n",
      "Epoch 2/2\n",
      "50407/50407 [==============================] - 3826s 76ms/step - loss: 0.1473 - acc: 0.9366 - val_loss: 0.1361 - val_acc: 0.9503\n"
     ]
    }
   ],
   "source": [
    "\"\"\"def getpickles():\n",
    "    train_X=pickle.load(open(\"glove/train_X.p\",'rb'))\n",
    "    dev_X=pickle.load(open(\"glove/dev_X.p\",'rb'))\n",
    "    train_labels=pickle.load(open(\"glove/train_labels.p\",'rb'))\n",
    "    dev_labels=pickle.load(open(\"glove/dev_labels.p\",\"rb\"))\n",
    "    return train_X,dev_X,train_labels,dev_labels\n",
    "\n",
    "train_X,dev_X,train_labels,dev_labels=getpickles()\n",
    "embeddings = get_embeddings(nlp.vocab)\n",
    "model,embeddings = compile_lstm(embeddings, {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': 5},\n",
    "             {'dropout': dropout, 'lr': learn_rate}\n",
    "             )\n",
    "newhistory=model.fit(train_X, train_labels, validation_split=0.3,\n",
    "              nb_epoch=2, batch_size=batch_size)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(newhistory.history['acc'])\n",
    "plt.plot(newhistory.history['val_acc'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.save('../TransLearn/models/transmodel1')\n",
    "model.save_weights('../Translearn/transmodel1_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "newmodel,embeddings=compileLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_dataFrame(dirpath):\n",
    "    #A substitute for read_data function.\n",
    "    dflist=[]\n",
    "    for filename in glob.glob(dirpath+\"train.tsv\"):\n",
    "        dflist.append(pd.read_csv(filename,sep='\\t',header=0))\n",
    "    traindf=pd.concat(dflist)\n",
    "    traindf.describe()\n",
    "    examples=[]\n",
    "    for indx,row in traindf.iterrows():\n",
    "        label=row['Sentiment']\n",
    "        sentence=row['Phrase']\n",
    "        examples.append((sentence,label))\n",
    "    #print(traindf.columns)\n",
    "    #labels=traindf['Sentiment'].values\n",
    "    #train_labels=to_categorical(numpy.asarray(labels),num_classes=5)\n",
    "    return zip(*examples)\n",
    "    #return traindf\n",
    "    \n",
    "    \n",
    "def TransferLearn(embeddings,trainable,labels,model=None,dirpath=\"taggedCalls/\",usinginputs=False):\n",
    "    #oldmodel=keras.model.load_model(\"modelpath\")\n",
    "    model_dir=None\n",
    "    train_dir=None\n",
    "    dev_dir=None\n",
    "    is_runtime=False\n",
    "    nr_hidden=256\n",
    "    max_length=50; # Shape\n",
    "    dropout=0.5\n",
    "    learn_rate=0.001; # General NN config\n",
    "    nb_epoch=3\n",
    "    batch_size=64\n",
    "    nr_examples=2000\n",
    "    if not usinginputs:\n",
    "        train_texts,train_labels=read_dataFrame(dirpath)\n",
    "    else:\n",
    "        train_texts=\"Scenes bro no input\"\n",
    "        train_labels=\"Scenes bro no labels\"\n",
    "    model.summary()\n",
    "    bwglstm,bwghistory = train(train_texts, train_labels,\"\",\"\", \n",
    "             {'nr_hidden': nr_hidden, 'max_length': max_length, 'nr_class': 5},\n",
    "             {'dropout': dropout, 'lr': learn_rate},\n",
    "             {},\n",
    "                         \n",
    "             nb_epoch=nb_epoch, batch_size=batch_size,by_sentence=True,model=model,embeddings=embeddings,trainable=trainable,labels=labels,usinginputs=usinginputs)\n",
    "    return lstm,history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 321291300 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 50, 300)           321291300 \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 50, 256)           76800     \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 512)               1050624   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 5)                 2565      \n",
      "=================================================================\n",
      "Total params: 322,421,289\n",
      "Trainable params: 2,565\n",
      "Non-trainable params: 322,418,724\n",
      "_________________________________________________________________\n",
      "Loading spaCy\n",
      "Train on 109242 samples, validate on 46818 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/models.py:981: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  warnings.warn('The `nb_epoch` argument in `fit` '\n",
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/engine/training.py:975: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n",
      "  'Discrepancy between trainable weights and collected trainable'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109242/109242 [==============================] - 7750s 71ms/step - loss: 0.8506 - acc: 0.6477 - val_loss: 0.8993 - val_acc: 0.6253\n",
      "Epoch 2/3\n",
      "109242/109242 [==============================] - 7281s 67ms/step - loss: 0.7163 - acc: 0.7020 - val_loss: 0.9288 - val_acc: 0.6211\n",
      "Epoch 3/3\n",
      " 72832/109242 [===================>..........] - ETA: 40:20 - loss: 0.6488 - acc: 0.7279"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-63ea052a9bb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbwgbasemodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m bwlstm,history=TransferLearn(model=bwgbasemodel,dirpath=\"rottentomatoes/data/sentiment/\",\n\u001b[0;32m---> 10\u001b[0;31m                              embeddings=embeddings,trainable=False,labels=\"0to4\",usinginputs=True)\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mbwlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../TransLearn/models/rotomato_frozenlayers'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mbwlstm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../TransLearn/models/rotomato_frozenlayerwts.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a64efcede48a>\u001b[0m in \u001b[0;36mTransferLearn\u001b[0;34m(embeddings, trainable, labels, model, dirpath, usinginputs)\u001b[0m\n\u001b[1;32m     43\u001b[0m              \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m              nb_epoch=nb_epoch, batch_size=batch_size,by_sentence=True,model=model,embeddings=embeddings,trainable=trainable,labels=labels,usinginputs=usinginputs)\n\u001b[0m\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-2fa086ed7a0c>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_texts, train_labels, dev_texts, dev_labels, lstm_shape, lstm_settings, lstm_optimizer, batch_size, nb_epoch, by_sentence, model, embeddings, trainable, labels, usinginputs)\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../TransLearn/2calls/bctrain_labels.p\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m     history=model.fit(train_X, train_labels, validation_split=0.3,\n\u001b[0;32m--> 279\u001b[0;31m               nb_epoch=nb_epoch, batch_size=batch_size)\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shape of embeddings'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1002\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1003\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1703\u001b[0m                               \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1704\u001b[0m                               \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1705\u001b[0;31m                               validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1706\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1707\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m   1234\u001b[0m                         \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1235\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1236\u001b[0;31m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1237\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                         \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2480\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2481\u001b[0m         updated = session.run(fetches=fetches, feed_dict=feed_dict,\n\u001b[0;32m-> 2482\u001b[0;31m                               **self.session_kwargs)\n\u001b[0m\u001b[1;32m   2483\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2484\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    898\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 900\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    901\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1135\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1136\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1137\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1315\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1316\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1317\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1318\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1324\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1305\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1306\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1307\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1407\u001b[0m       return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1408\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1409\u001b[0;31m           run_metadata)\n\u001b[0m\u001b[1;32m   1410\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1411\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_exception_on_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#My Attempt to train for rotomato\n",
    "\n",
    "import keras\n",
    "embeddings=pickle.load(open('../TransLearn/2calls/TLembeddings.p','rb'))\n",
    "bwgbasemodel=keras.models.load_model(\"../TransLearn/models/TLModel.p\")\n",
    "for layer in bwgbasemodel.layers:\n",
    "    layer.trainable=False\n",
    "bwgbasemodel.layers[-1].trainable=True\n",
    "bwlstm,history=TransferLearn(model=bwgbasemodel,dirpath=\"rottentomatoes/data/sentiment/\",\n",
    "                             embeddings=embeddings,trainable=False,labels=\"0to4\",usinginputs=True)\n",
    "bwlstm.save('../TransLearn/models/rotomato_frozenlayers')\n",
    "bwlstm.save_weight('../TransLearn/models/rotomato_frozenlayerwts.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prashanththekkada/.virtualenvs/dl4cv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:97: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 321291300 elements. This may consume a large amount of memory.\n",
      "  num_elements)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dev_texts' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b80cb3be929e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../TransLearn/2calls/TLembeddings.p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mbwgbasemodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../TransLearn/models/TLModel.p\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlstm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTransferLearn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbwgbasemodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"taggedCalls/\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0to4\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-0b1631786811>\u001b[0m in \u001b[0;36mTransferLearn\u001b[0;34m(embeddings, trainable, labels, model, dirpath)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mnr_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mtrain_texts\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mread_dataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     bwglstm,bwghistory = train(train_texts, train_labels, dev_texts, dev_labels,\n\u001b[0m\u001b[1;32m     36\u001b[0m              \u001b[0;34m{\u001b[0m\u001b[0;34m'nr_hidden'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnr_hidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'max_length'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nr_class'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m              \u001b[0;34m{\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'lr'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlearn_rate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dev_texts' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"import keras\n",
    "embeddings=pickle.load(open('../TransLearn/2calls/TLembeddings.p','rb'))\n",
    "bwgbasemodel=keras.models.load_model(\"../TransLearn/models/TLModel.p\")\n",
    "\n",
    "lstm,history=TransferLearn(model=bwgbasemodel,dirpath=\"taggedCalls/\",embeddings=embeddings,trainable=True,labels=\"0to4\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readTomatoes(dirpath):\n",
    "    traindf = pd.read_csv('rottentomatoes/train.tsv', sep='\\t', header=0)\n",
    "    examples=[]\n",
    "    for indx,row in traindf.iterrows():\n",
    "        label=row['Sentiment']\n",
    "        sentence=row['Phrase']\n",
    "        examples.append((sentence,label))\n",
    "    #print(traindf.columns)\n",
    "    #labels=traindf['Sentiment'].values\n",
    "    #train_labels=to_categorical(numpy.asarray(labels),num_classes=5)\n",
    "    return zip(*examples)\n",
    "    #return traindf\n",
    "def readLMDict(dirpath):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "print(getFinSent.getSentiment(\"rise with improvements in increase\",negList,posList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['able',\n",
       " 'abundance',\n",
       " 'abundant',\n",
       " 'acclaimed',\n",
       " 'accomplish',\n",
       " 'accomplished',\n",
       " 'accomplishes',\n",
       " 'accomplishing',\n",
       " 'accomplishment',\n",
       " 'accomplishments',\n",
       " 'achieve',\n",
       " 'achieved',\n",
       " 'achievement',\n",
       " 'achievements',\n",
       " 'achieves',\n",
       " 'achieving',\n",
       " 'adequately',\n",
       " 'advancement',\n",
       " 'advancements',\n",
       " 'advances',\n",
       " 'advancing',\n",
       " 'advantage',\n",
       " 'advantaged',\n",
       " 'advantageous',\n",
       " 'advantageously',\n",
       " 'advantages',\n",
       " 'alliance',\n",
       " 'alliances',\n",
       " 'assure',\n",
       " 'assured',\n",
       " 'assures',\n",
       " 'assuring',\n",
       " 'attain',\n",
       " 'attained',\n",
       " 'attaining',\n",
       " 'attainment',\n",
       " 'attainments',\n",
       " 'attains',\n",
       " 'attractive',\n",
       " 'attractiveness',\n",
       " 'beautiful',\n",
       " 'beautifully',\n",
       " 'beneficial',\n",
       " 'beneficially',\n",
       " 'benefit',\n",
       " 'benefited',\n",
       " 'benefiting',\n",
       " 'benefitted',\n",
       " 'benefitting',\n",
       " 'best',\n",
       " 'better',\n",
       " 'bolstered',\n",
       " 'bolstering',\n",
       " 'bolsters',\n",
       " 'boom',\n",
       " 'booming',\n",
       " 'boost',\n",
       " 'boosted',\n",
       " 'breakthrough',\n",
       " 'breakthroughs',\n",
       " 'brilliant',\n",
       " 'charitable',\n",
       " 'collaborate',\n",
       " 'collaborated',\n",
       " 'collaborates',\n",
       " 'collaborating',\n",
       " 'collaboration',\n",
       " 'collaborations',\n",
       " 'collaborative',\n",
       " 'collaborator',\n",
       " 'collaborators',\n",
       " 'compliment',\n",
       " 'complimentary',\n",
       " 'complimented',\n",
       " 'complimenting',\n",
       " 'compliments',\n",
       " 'conclusive',\n",
       " 'conclusively',\n",
       " 'conducive',\n",
       " 'confident',\n",
       " 'constructive',\n",
       " 'constructively',\n",
       " 'courteous',\n",
       " 'creative',\n",
       " 'creatively',\n",
       " 'creativeness',\n",
       " 'creativity',\n",
       " 'delight',\n",
       " 'delighted',\n",
       " 'delightful',\n",
       " 'delightfully',\n",
       " 'delighting',\n",
       " 'delights',\n",
       " 'dependability',\n",
       " 'dependable',\n",
       " 'desirable',\n",
       " 'desired',\n",
       " 'despite',\n",
       " 'destined',\n",
       " 'diligent',\n",
       " 'diligently',\n",
       " 'distinction',\n",
       " 'distinctions',\n",
       " 'distinctive',\n",
       " 'distinctively',\n",
       " 'distinctiveness',\n",
       " 'dream',\n",
       " 'easier',\n",
       " 'easily',\n",
       " 'easy',\n",
       " 'effective',\n",
       " 'efficiencies',\n",
       " 'efficiency',\n",
       " 'efficient',\n",
       " 'efficiently',\n",
       " 'empower',\n",
       " 'empowered',\n",
       " 'empowering',\n",
       " 'empowers',\n",
       " 'enable',\n",
       " 'enabled',\n",
       " 'enables',\n",
       " 'enabling',\n",
       " 'encouraged',\n",
       " 'encouragement',\n",
       " 'encourages',\n",
       " 'encouraging',\n",
       " 'enhance',\n",
       " 'enhanced',\n",
       " 'enhancement',\n",
       " 'enhancements',\n",
       " 'enhances',\n",
       " 'enhancing',\n",
       " 'enjoy',\n",
       " 'enjoyable',\n",
       " 'enjoyably',\n",
       " 'enjoyed',\n",
       " 'enjoying',\n",
       " 'enjoyment',\n",
       " 'enjoys',\n",
       " 'enthusiasm',\n",
       " 'enthusiastic',\n",
       " 'enthusiastically',\n",
       " 'excellence',\n",
       " 'excellent',\n",
       " 'excelling',\n",
       " 'excels',\n",
       " 'exceptional',\n",
       " 'exceptionally',\n",
       " 'excited',\n",
       " 'excitement',\n",
       " 'exciting',\n",
       " 'exclusive',\n",
       " 'exclusively',\n",
       " 'exclusiveness',\n",
       " 'exclusives',\n",
       " 'exclusivity',\n",
       " 'exemplary',\n",
       " 'fantastic',\n",
       " 'favorable',\n",
       " 'favorably',\n",
       " 'favored',\n",
       " 'favoring',\n",
       " 'favorite',\n",
       " 'favorites',\n",
       " 'friendly',\n",
       " 'gain',\n",
       " 'gained',\n",
       " 'gaining',\n",
       " 'gains',\n",
       " 'good',\n",
       " 'great',\n",
       " 'greater',\n",
       " 'greatest',\n",
       " 'greatly',\n",
       " 'greatness',\n",
       " 'happiest',\n",
       " 'happily',\n",
       " 'happiness',\n",
       " 'happy',\n",
       " 'highest',\n",
       " 'honor',\n",
       " 'honorable',\n",
       " 'honored',\n",
       " 'honoring',\n",
       " 'honors',\n",
       " 'ideal',\n",
       " 'impress',\n",
       " 'impressed',\n",
       " 'impresses',\n",
       " 'impressing',\n",
       " 'impressive',\n",
       " 'impressively',\n",
       " 'improve',\n",
       " 'improved',\n",
       " 'improvement',\n",
       " 'improvements',\n",
       " 'improves',\n",
       " 'improving',\n",
       " 'incredible',\n",
       " 'incredibly',\n",
       " 'influential',\n",
       " 'informative',\n",
       " 'ingenuity',\n",
       " 'innovate',\n",
       " 'innovated',\n",
       " 'innovates',\n",
       " 'innovating',\n",
       " 'innovation',\n",
       " 'innovations',\n",
       " 'innovative',\n",
       " 'innovativeness',\n",
       " 'innovator',\n",
       " 'innovators',\n",
       " 'insightful',\n",
       " 'inspiration',\n",
       " 'inspirational',\n",
       " 'integrity',\n",
       " 'invent',\n",
       " 'invented',\n",
       " 'inventing',\n",
       " 'invention',\n",
       " 'inventions',\n",
       " 'inventive',\n",
       " 'inventiveness',\n",
       " 'inventor',\n",
       " 'inventors',\n",
       " 'leadership',\n",
       " 'leading',\n",
       " 'loyal',\n",
       " 'lucrative',\n",
       " 'meritorious',\n",
       " 'opportunities',\n",
       " 'opportunity',\n",
       " 'optimistic',\n",
       " 'outperform',\n",
       " 'outperformed',\n",
       " 'outperforming',\n",
       " 'outperforms',\n",
       " 'perfect',\n",
       " 'perfected',\n",
       " 'perfectly',\n",
       " 'perfects',\n",
       " 'pleasant',\n",
       " 'pleasantly',\n",
       " 'pleased',\n",
       " 'pleasure',\n",
       " 'plentiful',\n",
       " 'popular',\n",
       " 'popularity',\n",
       " 'positive',\n",
       " 'positively',\n",
       " 'preeminence',\n",
       " 'preeminent',\n",
       " 'premier',\n",
       " 'premiere',\n",
       " 'prestige',\n",
       " 'prestigious',\n",
       " 'proactive',\n",
       " 'proactively',\n",
       " 'proficiency',\n",
       " 'proficient',\n",
       " 'proficiently',\n",
       " 'profitability',\n",
       " 'profitable',\n",
       " 'profitably',\n",
       " 'progress',\n",
       " 'progressed',\n",
       " 'progresses',\n",
       " 'progressing',\n",
       " 'prospered',\n",
       " 'prospering',\n",
       " 'prosperity',\n",
       " 'prosperous',\n",
       " 'prospers',\n",
       " 'rebound',\n",
       " 'rebounded',\n",
       " 'rebounding',\n",
       " 'receptive',\n",
       " 'regain',\n",
       " 'regained',\n",
       " 'regaining',\n",
       " 'resolve',\n",
       " 'revolutionize',\n",
       " 'revolutionized',\n",
       " 'revolutionizes',\n",
       " 'revolutionizing',\n",
       " 'reward',\n",
       " 'rewarded',\n",
       " 'rewarding',\n",
       " 'rewards',\n",
       " 'satisfaction',\n",
       " 'satisfactorily',\n",
       " 'satisfactory',\n",
       " 'satisfied',\n",
       " 'satisfies',\n",
       " 'satisfy',\n",
       " 'satisfying']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "posList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "177\n",
      "hey\n",
      "230\n",
      "there\n",
      "79\n",
      ".\n",
      "477\n",
      "My\n",
      "662\n",
      "name\n",
      "137\n",
      "is\n",
      "759897\n",
      "goku\n",
      "79\n",
      ".\n",
      "132\n",
      "I\n",
      "398\n",
      "am\n",
      "130\n",
      "a\n",
      "135240\n",
      "disco\n",
      "78455\n",
      "dancer\n",
      "=========================\n",
      "177\n",
      "hey\n",
      "230\n",
      "there\n",
      "79\n",
      ".\n",
      "477\n",
      "My\n",
      "662\n",
      "name\n",
      "137\n",
      "is\n",
      "759897\n",
      "goku\n",
      "79\n",
      ".\n",
      "132\n",
      "I\n",
      "398\n",
      "am\n",
      "130\n",
      "a\n",
      "135240\n",
      "disco\n",
      "78455\n",
      "dancer\n"
     ]
    }
   ],
   "source": [
    "g=nlp('hey there. My name is goku. I am a disco dancer')\n",
    "for token in g:\n",
    "    print(token.vocab.vectors.find(key=token.orth))\n",
    "    print(token)\n",
    "print(\"=\"*25)\n",
    "temp=nlp(g.text)\n",
    "for token in temp:\n",
    "    print(token.vocab.vectors.find(key=token.orth))\n",
    "    print(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "set_extension() takes exactly 1 positional argument (2 given)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-87de95bb1f09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokens\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mToken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_extension\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'force'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mtoken.pyx\u001b[0m in \u001b[0;36mspacy.tokens.token.Token.set_extension\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: set_extension() takes exactly 1 positional argument (2 given)"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Token\n",
    "Token.set_extension('force', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"hey,there he. kill's me.\"]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=\"hey,there he. kill's me.\"\n",
    "k.split(' .,')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "type(to_categorical([3,2,1],4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
