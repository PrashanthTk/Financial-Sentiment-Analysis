{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-10 23:45:34,290 : INFO : 'pattern' package not found; tag filters are not available for English\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"Human machine interface for lab abc computer applications\",\n",
    "             \"A survey of user opinion of computer system response time\",\n",
    "               \"Tk is a god\",\n",
    "               \"Black panther is a legend\",\n",
    "               \"Stocks have fallen after a bitcoin crash\",\n",
    "               \"Finance markets are improving\",\n",
    "               \"The intersection graph of paths in trees\",\n",
    "               \"Saddam was killed in a drone strike\",\n",
    "               \"New York is a beautiful place ?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''dictionary = corpora.Dictionary.load('/tmp/deerwester.dict')\n",
    "corpus = corpora.MmCorpus('/tmp/deerwester.mm')\n",
    "print(corpus)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sense2vec import Sense2VecComponent\n",
    "nlp = spacy.load('en')  # make sure to use larger model!\n",
    "s2v=Sense2VecComponent('reddit_vectors-1.1.0')\n",
    "nlp.add_pipe(s2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp=nlp(u'Yo New York.i like this song more than my food')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([], shape=(0, 0), dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs=nlp.vocab.vectors.data\n",
    "embs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yo New York.i\n"
     ]
    }
   ],
   "source": [
    "for ent in temp.ents:\n",
    "    print(ent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.591487447296181\n"
     ]
    }
   ],
   "source": [
    "\n",
    "doc1 = nlp(u'Hello, world. Here are two sentences.')\n",
    "#list(en_doc.sents) lists the sentences\n",
    "doc2=nlp(u'the poutine is c')\n",
    "#extra functionalities \n",
    "print(doc1.similarity(doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_word(word):\n",
    "    queries = [w for w in word.vocab if w.is_lower == word.is_lower and w.prob >= -15]\n",
    "    by_similarity = sorted(queries, key=lambda w: word.similarity(w), reverse=True)\n",
    "    return by_similarity[:10]\n",
    "    [w.lower_ for w in most_similar(nlp.vocab[u'dog'])]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0.5012605994794488, Stocks have fallen after a bitcoin crash),\n",
       " (0.48131073275161906,\n",
       "  Human machine interface for lab abc computer applications),\n",
       " (0.47227721727903144, Saddam was killed in a drone strike),\n",
       " (0.41934360691694567,\n",
       "  A survey of user opinion of computer system response time),\n",
       " (0.3916011385701601, Finance markets are improving)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for a segment (question or query or topic segment), return the most 5 most relevant pieces of information\n",
    "#returns a list of 5 tuples: (score,sentence)\n",
    "#params: raw text for sentences and query\n",
    "def getRelevantInfo(query,translist,num_responses=5):\n",
    "    scores=[]\n",
    "    responses={}\n",
    "    query=nlp(query)\n",
    "    translist=[nlp(x) for x in translist]\n",
    "    for segment in translist:\n",
    "        scores.append((query.similarity(segment),segment))\n",
    "    scores=sorted(scores,key=lambda x: x[0],reverse=True)\n",
    "    return scores[:5]\n",
    "getRelevantInfo('colored animal ',documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bz2\n",
    "source_file='glove2.txt'\n",
    "tarbz2contents = bz2.compress(open(source_file, 'rb').read(), 9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with bz2.open(\"glove/glove2.txt.bz2\", \"wt\") as bz_file:\n",
    "    bz_file.write(str(tarbz2contents))\n",
    "    bz_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotADirectoryError",
     "evalue": "[Errno 20] Not a directory: 'glove/glove2.txt.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotADirectoryError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-5e400f37ac77>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_glove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'glove/glove2.txt.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mvectors.pyx\u001b[0m in \u001b[0;36mspacy.vectors.Vectors.from_glove\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36miterdir\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1078\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1079\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1080\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'.'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'..'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m                 \u001b[0;31m# Yielding a path object for these makes little sense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python/3.6.5/Frameworks/Python.framework/Versions/3.6/lib/python3.6/pathlib.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(pathobj, *args)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0mfunctools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstrfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mstrfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mstaticmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwrapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotADirectoryError\u001b[0m: [Errno 20] Not a directory: 'glove/glove2.txt.bin'"
     ]
    }
   ],
   "source": [
    "nlp.vocab.vectors.from_glove('glove/glove2.txt.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'spacy.vocab' has no attribute 'write_binary_vectors'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-385cd063568e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_rep_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbinary_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mset_spacy_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbinary_loc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbz2_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-34-385cd063568e>\u001b[0m in \u001b[0;36mset_spacy_vectors\u001b[0;34m(nlp, binary_loc, bz2_loc)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mset_spacy_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbz2_loc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbz2_loc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mspacy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_binary_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbz2_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mwrite_binary_vectors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbz2_loc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_loc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'spacy.vocab' has no attribute 'write_binary_vectors'"
     ]
    }
   ],
   "source": [
    "import spacy.vocab\n",
    "binary_loc='glove/glove2.txt.bin'\n",
    "bz2_loc='glove2.txt'\n",
    "def set_spacy_vectors(nlp, binary_loc, bz2_loc=None):\n",
    "    if bz2_loc is not None:\n",
    "        spacy.vocab.write_binary_vectors(bz2_loc, binary_loc)\n",
    "    write_binary_vectors(bz2_loc, binary_loc)\n",
    "\n",
    "    nlp.vocab.load_rep_vectors(binary_loc)\n",
    "set_spacy_vectors(nlp,binary_loc,bz2_loc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nlp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-4d94b1f16779>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtrans\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fullText/faa4d712-5438-418b-8415-ab6eae7e59a2.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtopics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'topics/topics_faa4d712-5438-418b-8415-ab6eae7e59a2.txt'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0msentchunks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msents\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nlp' is not defined"
     ]
    }
   ],
   "source": [
    "trans=open('fullText/faa4d712-5438-418b-8415-ab6eae7e59a2.txt','r')\n",
    "topics=open('topics/topics_faa4d712-5438-418b-8415-ab6eae7e59a2.txt','r')\n",
    "doc = nlp(trans.read())\n",
    "sentchunks=[]\n",
    "for sent in doc.sents:\n",
    "    sentchunks.append(sent.text)\n",
    "topiclist=[]\n",
    "for topic in topics:\n",
    "    topiclist.append(topic)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sentchunks' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-7053e25f4d28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentchunks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'sentchunks' is not defined"
     ]
    }
   ],
   "source": [
    "print(len(max(sentchunks,key=len).split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for topic in topiclist:\n",
    "    responses=getRelevantInfo(topic,sentchunks)\n",
    "    print('='*10)\n",
    "    print('Topic:'+topic)\n",
    "\n",
    "    for resp in responses:\n",
    "        print('Score:'+str(resp[0]))\n",
    "        print(resp[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aylienapiclient import textapi\n",
    "aylienclient = textapi.Client(CREDS:CREDS)\n",
    "sentiment = aylienclient.Sentiment({'text': 'We see a possible rise in share prices over next month'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testAylienAcc(aylienclient,texts,targets):\n",
    "    if(len(texts)<60):\n",
    "        labels=[]\n",
    "        counter=0\n",
    "        score=0\n",
    "        for text in texts:\n",
    "            print(text)\n",
    "            sentiment=aylienclient.Sentiment({'text':text})\n",
    "            pval=sentiment['polarity']\n",
    "            confidence=sentiment['polarity_confidence']\n",
    "            if pval=='neutral':\n",
    "                labels.append(2)\n",
    "            if pval=='positive':\n",
    "                #if float(confidence)>0.75:\n",
    "                    labels.append(3)\n",
    "                else:\n",
    "                    labels.append(3)\n",
    "            if pval=='negative':\n",
    "                if float(confidence)>0.75:\n",
    "                    labels.append(0)\n",
    "                else:\n",
    "                    labels.append(1)\n",
    "            if targets[counter]==labels[counter]:\n",
    "                score+=1\n",
    "        print('Accuracy of Aylien API'+str(float(100*score/len(targets))))\n",
    "        return list(zip(texts,labels))\n",
    "\n",
    "\n",
    "texts=[\"it's the split of interest between between Bitcoin and alternative cryptoassets obviously aetherium with the has benefited tremendously with from the Ico from the Ico push there's been a lot of conversation about Bitcoin cash for transactional work and just general interest a lot of security coins\",\n",
    "       \"Bank in large investors stepping into the game yesterday\",\n",
    "       \"but I think it has the same fundamental problem of Bitcoin house and said they have not addressed the on protocol scaling well the same timing chain security report consensus make bacon into entirely\",\n",
    "       \"anywhere from 10 to 15% one store said that they actually thought it was close to 50% of their customers are actually staging transactions now now I will say that she said to me I wrote we picked up a huge number of customers for whatever reason the staging isn't available at other cars with steam some accretive volume and some other areas but generally speaking are volume is up\",\n",
    "       \"I think that the global exclusivity erosion is hurt them to Mexico all the way to Russia\",\n",
    "       \"I think that one of Western Union biggest problem is that they're never proactive they're always reacting to things and sort of keeping things status quo you look at if I write off a bunch of things\"\n",
    "       \"They have been winning since last season and their eyes are set on the championship\",\n",
    "       \"Her condition isn't improving and neither is her environment\",\n",
    "       \"Nons e . asdin noise data rubas sdadd\",\n",
    "       \"This is terrible news because of the litigations and layoffs\"]    \n",
    "\n",
    "labels=[4,3,1,3,0,0,3,1,2,2,0]\n",
    "testAylienAcc(aylienclient,texts,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('asdsd', 1), ('asd', 2)]\n"
     ]
    }
   ],
   "source": [
    "l=['asdsd','asd']\n",
    "k=[1,2]\n",
    "g=list(zip(l,k))\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'zip' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-73ec0726ff88>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'zip' object is not subscriptable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(384,)\n"
     ]
    }
   ],
   "source": [
    "temp=nlp(u\"Winsome TD is a  new entrance but I don't think the world needs any more entrance how did thank you so much and for 18 minutes did not tell you different business lines but they run rfps for large Grocers Regional Grocers retailers department store gift cards and etcetera.\")\n",
    "print(temp.vector.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes only 50 text chunks at once\n",
    "def Call_Sentiment_Tagger(translist):\n",
    "    numtrans=50\n",
    "    sent_list=[]\n",
    "    if(len(translist)<50):\n",
    "        numtrans=len(translist)-1\n",
    "    for index in range(numtrans):\n",
    "        sentiment = aylienclient.Sentiment({'text': translist[index]})\n",
    "        sent_list.append(sentiment)\n",
    "    return sent_list\n",
    "\n",
    "def AvgSentiment(translist):\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans=open('fullText/faa4d712-5438-418b-8415-ab6eae7e59a2.txt','r')\n",
    "topics=open('topics/topics_faa4d712-5438-418b-8415-ab6eae7e59a2.txt','r')\n",
    "doc = nlp(trans.read())\n",
    "sentchunks=[]\n",
    "for sent in doc.sents:\n",
    "    sentchunks.append(sent.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(384,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testx=[nlp(doc).vector for doc in sentchunks]\n",
    "testx[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "testmodel=keras.models.load_model('model/imdb384model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "doc=nlp(open('fullText/6076108d-971c-4988-ba05-4712c94146b3.txt','r').read())\n",
    "sents=[]\n",
    "for sent in doc.sents:\n",
    "    sents.append([sent])\n",
    "sents[:3]\n",
    "with open(\"taggedCalls/6076108d-971c-4988-ba05-4712c94146b3.csv\", \"w\") as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerows(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(open('fullText/445c28bb-8734-451c-af97-f812853ea989.txt','r').read())\n",
    "print('Entity Name: \\t Entity Type ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity \t \t Relevance\n",
      "Western Union\t43\n",
      "Walmart\t26\n",
      "one\t15\n",
      "India\t11\n",
      "MoneyGram\t9\n",
      "today\t4\n",
      "PayPal\t4\n",
      "Mexico\t4\n",
      "three\t4\n",
      "two\t4\n",
      "many years\t3\n",
      "US\t3\n",
      "20\t3\n",
      "6 months ago\t3\n",
      "RSVP\t2\n",
      "Western Union agent\t2\n",
      "5 years ago\t2\n",
      "200\t2\n",
      "Corridor\t2\n",
      "Carter\t2\n",
      "Europe\t2\n",
      "3\t2\n",
      "five\t2\n",
      "first\t2\n",
      "MoneyGram Walmart\t2\n",
      "5\t2\n",
      "12 months\t2\n",
      "1\t2\n",
      "a Thousand\t2\n",
      "10\t2\n",
      "two to three\t2\n",
      "ten\t2\n",
      "International\t2\n",
      "Jackson\t1\n",
      "the past 12 months\t1\n",
      "later than 4 p.m. Eastern questions\t1\n",
      "WG strategy.com\t1\n",
      "J's\t1\n",
      "W G's\t1\n",
      "550\t1\n",
      "Financial Service products\t1\n",
      "first event hybrids\t1\n",
      "Legacy agent business\t1\n",
      "25% money transfer volume\t1\n",
      "six\t1\n",
      "Western Unions volume\t1\n",
      "Scott\t1\n",
      "Dental companies\t1\n",
      "the last five years\t1\n",
      "New York State\t1\n",
      "Biggest Loser\t1\n",
      "Love Hedwig\t1\n",
      "Institue\t1\n",
      "up every year\t1\n",
      "Asian store\t1\n",
      "u.s\t1\n",
      "Dominican Republic\t1\n",
      "quarter\t1\n",
      "14 months\t1\n",
      "quarter-by-quarter\t1\n",
      "Maria Sandpoint\t1\n",
      "the day\t1\n",
      "Syria\t1\n",
      "Jame\t1\n",
      "a little better days\t1\n",
      "Global Banks\t1\n",
      "Citibank\t1\n",
      "Walmart MoneyGram\t1\n",
      "RIA\t1\n",
      "Tuesday\t1\n",
      "two years\t1\n",
      "New York\t1\n",
      "Mosel\t1\n",
      "Western Union volumes\t1\n",
      "South America\t1\n",
      "Africa\t1\n",
      "4\t1\n",
      "58\t1\n",
      "51\t1\n",
      "a few years\t1\n",
      "this 10 years\t1\n",
      "every year\t1\n",
      "J&J\t1\n",
      "25%\t1\n",
      "Pat\t1\n",
      "Dan Truman\t1\n",
      "Kenwood Pay-O-Matic\t1\n",
      "New York City\t1\n",
      "Preston unifresh checks\t1\n",
      "the next one to two years\t1\n",
      "this 3 to 6 months ago\t1\n",
      "a 6 to 9 months ago\t1\n",
      "MoneyGram Aunt\t1\n",
      "Yale\t1\n",
      "a minute\t1\n",
      "2018\t1\n",
      "Elsa\t1\n",
      "New York area\t1\n",
      "Amaya love\t1\n",
      "Upstate New York\t1\n",
      "Shirley Drive\t1\n",
      "the last 12 months\t1\n",
      "2010 to 2017\t1\n",
      "the first quarter 2018\t1\n",
      "five to seven dollars\t1\n",
      "last year\t1\n",
      "5%\t1\n",
      "2017\t1\n",
      "the last couple of days\t1\n",
      "10 to 15%\t1\n",
      "close to 50%\t1\n",
      "Sarah\t1\n",
      "10 or $15\t1\n",
      "Western Union side\t1\n",
      "Western Union Union\t1\n",
      "Western Union fees\t1\n",
      "Ice-T's\t1\n",
      "Sand\t1\n",
      "Century hundred dollars\t1\n",
      "500\t1\n",
      "15\t1\n",
      "ten dollars\t1\n",
      "Western Union commission\t1\n",
      "every every month\t1\n",
      "Walmart MoneyGram MoneyGram\t1\n",
      "7\t1\n",
      "20 or $30\t1\n",
      "Panacea\t1\n",
      "Walmart to World\t1\n",
      "MoneyGram customer\t1\n",
      "State\t1\n",
      "around $5\t1\n",
      "the age of 60\t1\n",
      "2\t1\n",
      "Core Business Road\t1\n",
      "2/3\t1\n",
      "Uber\t1\n",
      "Liberation\t1\n",
      "Egypt\t1\n",
      "Moe\t1\n",
      "ACH\t1\n",
      "Tampa\t1\n",
      "15 minutes\t1\n",
      "Remains\t1\n",
      "Bank\t1\n",
      "12-18 months\t1\n",
      "MoneyGram relationship\t1\n",
      "Ria\t1\n",
      "EFT Zell\t1\n",
      "Money Transfer\t1\n",
      "nine\t1\n",
      "the last 2-3 years\t1\n",
      "20 minutes\t1\n",
      "1/2\t1\n",
      "two-year time frame\t1\n",
      "Tech advisor\t1\n",
      "one to 10\t1\n",
      "Bill\t1\n",
      "a couple years\t1\n",
      "12 to 15 quarters\t1\n",
      "17\t1\n",
      "first time\t1\n",
      "three plus years\t1\n",
      "next week\t1\n",
      "Russia\t1\n",
      "2.5\t1\n",
      "NJ\t1\n",
      "the past 3 or 4 years\t1\n",
      "Christy Buchanon\t1\n",
      "FX\t1\n",
      "NSF\t1\n",
      "today implications\t1\n",
      "one second let\t1\n",
      "Igor\t1\n",
      "Nazi Gore\t1\n",
      "Continental exchange Solutions\t1\n",
      "12\t1\n",
      "18 months\t1\n",
      "the next 12 to 18 months\t1\n",
      "just a few minutes ago\t1\n",
      "Intex you\t1\n",
      "the next 12 to 18\t1\n",
      "Frenchy\t1\n",
      "European business\t1\n",
      "Colorado\t1\n",
      "Independence\t1\n",
      "four\t1\n",
      "40 to 45%\t1\n",
      "this month\t1\n",
      "Walmart International\t1\n",
      "Independence MO location\t1\n",
      "MoneyGrams margins\t1\n",
      "Express\t1\n",
      "a hundred fifty\t1\n",
      "seventy thousand\t1\n",
      "one to ten\t1\n",
      "about 7\t1\n",
      "North America\t1\n",
      "Africa Baby\t1\n",
      "Raging\t1\n",
      "the Mobile Transfer\t1\n",
      "Union\t1\n",
      "Gap\t1\n",
      "China\t1\n",
      "about $5\t1\n",
      "woot.com side\t1\n",
      "more than 55,000\t1\n",
      "ATM machine\t1\n",
      "India Express money\t1\n",
      "Bella\t1\n",
      "Black Market\t1\n",
      "the last 34 years\t1\n",
      "about 30 to 40 thousand\t1\n",
      "Express money\t1\n",
      "India portion\t1\n",
      "Network\t1\n",
      "90,000\t1\n",
      "Cassens\t1\n",
      "India Germany\t1\n",
      "Acquisitions\t1\n",
      "Eevee\t1\n",
      "150,000\t1\n",
      "first-mover advantage\t1\n",
      "about 3 minutes\t1\n",
      "Chris\t1\n",
      "Swank\t1\n",
      "around 4 or 5\t1\n",
      "Robin\t1\n",
      "Syntec\t1\n",
      "months ago\t1\n",
      "6\t1\n",
      "Old School solutions\t1\n",
      "Jay\t1\n",
      "Maria\t1\n",
      "Western Unions\t1\n",
      "the Ledger on the compliance and Regulatory\t1\n",
      "about 9 years ago one\t1\n",
      "Regulators\t1\n",
      "Western Union 1\t1\n",
      "1 gram\t1\n",
      "1 to 5 years\t1\n",
      "Kessler\t1\n",
      "about 3 years ago\t1\n",
      "Earth\t1\n",
      "MoneyGram world\t1\n",
      "the Business Solutions\t1\n",
      "World\t1\n",
      "World remit\t1\n",
      "Innovative\t1\n",
      "500000\t1\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "ent_counts={}\n",
    "for entity in doc.ents:\n",
    "    if entity.text in ent_counts:\n",
    "        ent_counts[entity.text]+=1\n",
    "    else:\n",
    "        ent_counts[entity.text]=1\n",
    "\n",
    "print('Entity \\t \\t Relevance')\n",
    "sorted_ents=sorted(ent_counts.items(), key=operator.itemgetter(1),reverse=True)\n",
    "for k,v in sorted_ents:\n",
    "    print(k+'\\t'+str(v))\n",
    "#print(entity.text+'\\t=====>'+entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "temp=nlp('Obama is a rox. Trump is a waste of my time')\n",
    "for token in temp:\n",
    "    print (token.sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
